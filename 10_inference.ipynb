{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc3c4861-bf18-4e2c-b49d-17fddeb0d988",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/COGS108/Lectures-Fa24/blob/master/10_inference.ipynb)\n",
    "\n",
    "## Inference\n",
    "\n",
    "- Association between variables: **Pearson's correlation coefficient** \n",
    "- Models: **Ordinary Linear Regression (OLS)**\n",
    "  - **simple linear regression**\n",
    "    - effect size\n",
    "    - p-value\n",
    "    - interpretation\n",
    "  - **multiple linear regression**\n",
    "    - confounding\n",
    "  - working with **transformed data**\n",
    "- Comparison of means: **Student's t test**\n",
    "\n",
    "\n",
    "\n",
    "To answer our quiz questions today please open this link in seperate browser tab or window:\n",
    "\n",
    "https://forms.gle/4mdyN7coMuurGR3H9\n",
    "\n",
    "We will be going back and forth from the notebook to this quiz several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17717c6-8af6-4b46-8240-11304b4d9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import seaborn and apply its plotting styles\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"white\", \n",
    "              font_scale=2, \n",
    "              rc={'axes.spines.right': False,'axes.spines.top': False}) # this last thing is equivalent to always having sns.despine()\n",
    "\n",
    "# import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# set plotting size parameter\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "# Statmodels & patsy\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from scipy.stats import pearsonr, norm, ttest_ind, skewnorm\n",
    "\n",
    "#\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "#improve resolution\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8cb4c-aa7e-420c-8e51-8bcb5a821500",
   "metadata": {},
   "source": [
    "# Correlation: Pearson correlation ($r$)\n",
    "\n",
    "- Measures the strength of association between two variables \n",
    "- Takes values [-1,1]\n",
    "- Positive Correlation \n",
    "    - $r$ > 0\n",
    "    - as $r$ approaches 1, x and y are highly correlated\n",
    "    - $r$ approaching 1 can happen with a shallow sloped line of regression\n",
    "    - $r$ approaching (not quite) 0 can happen with a steep sloped line of regression\n",
    "- Negative Correlation \n",
    "    - $r$ < 0\n",
    "    - as $r$ approaches -1, x and y are highly negatively correlated\n",
    "    - $r$ approaching -1 can happen with a shallow sloped line of regression\n",
    "    - $r$ approaching (not quite) 0 can happen with a steep sloped line of regression\n",
    "- Zero Correlation\n",
    "   - $r \\approx 0$\n",
    "   - as $r$ approaches 0, x and y are not correlated\n",
    "   - as $r$ approaches 0, the slope of the line of regression becomes 0 (horizontal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a9f78-465c-4af5-a3b0-70c5a8d08f65",
   "metadata": {},
   "source": [
    "## No Correlation example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c44897-f5ca-4601-89b2-7c060be15696",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# x and y are uncorrelated because they are independent draws of random numbers\n",
    "# the value of x does not depend on the corresponding value of y and vice versa\n",
    "x = np.random.randint(0, 50, 1000)\n",
    "y = np.random.randint(0, 50, 1000)\n",
    "\n",
    "plt.scatter(x, y, s=50);\n",
    "print('correlation coefficients:\\n', np.corrcoef(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34bc80-f865-459f-9b9b-54aa0b2aa35f",
   "metadata": {},
   "source": [
    "### Clicker Question\n",
    "\n",
    "What would you expect Pearson correlation for the following relationship to be? Note that the value of y is a function of x plus some random noise added in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd9835e-1eb5-4073-b682-32a3bc4a45d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting a random seed makes this answer repeatable\n",
    "np.random.seed(888)\n",
    "\n",
    "# 1000 random integers between 0 and 50\n",
    "x = np.random.randint(0, 50, 200)\n",
    "y = x + np.random.normal(0, 10, 200)\n",
    "\n",
    "plt.scatter(x, y, s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7761a17b-8815-4205-a086-ceb06d592b5b",
   "metadata": {},
   "source": [
    "- A) -0.8\n",
    "- B) -0.4\n",
    "- C) 0\n",
    "- D) 0.4\n",
    "- E) 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc3868e-3fc1-4b20-bb21-1cfd9086d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now calculate it here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4843a9cb-6fb3-4650-aedb-15e864526d13",
   "metadata": {},
   "source": [
    "## The Data\n",
    "Here is some data we will use to explore inference.  Both for regression models and t-tests.  \n",
    "\n",
    "The rows show (for a particular point in time) values for US states\n",
    "\n",
    "The columns show\n",
    "- Percent of people with incomes low enough to be classed as living in poverty\n",
    "- Birth rates for mothers who are in different age bands\n",
    "- An overall teen birth rate (combining different age bands)\n",
    "- A rate of violent crimes committed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04469d7-b287-4010-8854-744550d72b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in file; specify that it is tab-separated file\n",
    "df = ( pd.read_csv('https://raw.githubusercontent.com/shanellis/datasets/master/index.txt', sep='\\t')\n",
    "      .set_index('Location') )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c76fe19-fb15-4953-955a-bd1b335a6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to explore the data... time for EDA!\n",
    "\n",
    "# Look at univariate shape and stats\n",
    "# Look at relationships between variables... what do scatter plots look like? are the correlations between variables?\n",
    "# nb: you can use both pandas and numpy methods to examine correlations and summary stats.  Just pick one!\n",
    "# you can use pandas, matplotlib, and seaborn to do EDA graphs.  Just pick one!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd37792-f63a-431c-b1fa-2f59962914a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfd12409-9d0e-46fd-9525-e127f9c7f96e",
   "metadata": {},
   "source": [
    "### So, which two variables...\n",
    "would you like to explore with a linear regression?  I'm going to start with my own pick, but you can modify these cells to do what you want :)\n",
    "\n",
    "## Assumptions of Linear Regression\n",
    "\n",
    "1. Linear relationship\n",
    "2. No multicollinearity\n",
    "3. No auto-correlation\n",
    "4. Homoscedasticity\n",
    "\n",
    "Let's check these assumptions in the data!\n",
    "\n",
    "There's no multi colinearity becasue we are only exploring two variables.\n",
    "There's no auto-correlation because there's no time variable here\n",
    "\n",
    "NB: you can check for all of these issues with tools built into our regression package `statsmodels` https://www.statsmodels.org/stable/diagnostic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e09b8-fa64-4fb0-ae90-38ca8aea99be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check linearity and homo/heteroscasticity by eyeball using sns.lmplot here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d110dc-b4e6-401b-b3b8-73abd0871c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e792f12-c448-484a-917a-21693c405f1d",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c712d1-f974-43b9-b1ed-ef7e5f4787b5",
   "metadata": {},
   "source": [
    "$$outcome = \\beta_0 + \\beta_1*predictor$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972a15b-78b6-49ea-af4a-53795a088dcf",
   "metadata": {},
   "source": [
    "First, let's be sure we're on the same page about what our outcome is. Here, we're intererested in whether **Poverty Percentage (predictor)** impacts **Teen Birth Rate (outcome)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c4da4b-2cd1-46b9-961a-6f603df015f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "# import statsmodels.formula.api as smf\n",
    "\n",
    "# OLD SKOOL way to do this\n",
    "# We can specify our model matrix using `pastsy`.\n",
    "outcome, predictors = patsy.dmatrices('TeenBrth ~ PovPct', df)\n",
    "model = sm.OLS(outcome, predictors)\n",
    "\n",
    "\n",
    "# Its much nicer to use the MODERN formulae interface instead!\n",
    "# I suggest that you just always use this!\n",
    "model = smf.ols(formula='TeenBrth ~ PovPct', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5331d257-bb71-433a-95e0-756a1bc4c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "model. # tab complete to see all the things inside the model object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7c2dfd-10bb-485f-b8c1-cc3242ff8254",
   "metadata": {},
   "source": [
    "And, then we just have to fit the model and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595c28c-629c-4da4-bbc5-276ebdda50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit the model\n",
    "results = model.fit()\n",
    "\n",
    "## look at the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0bc3c-775e-42ec-9ed9-3b5a1c6e7fbc",
   "metadata": {},
   "source": [
    "### Question #1\n",
    "\n",
    "What is the effect size of the relationship between Poverty Percentage and Teen Birth Rate?\n",
    "\n",
    "- A) 15.67\n",
    "- B) 2.03\n",
    "- C) 4.032\n",
    "- D) 0.495"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabedcd3-a2b8-4fd3-9e54-286edbb03d96",
   "metadata": {},
   "source": [
    "There is a lot of information in there. Let's focus on the three pieces we discussed last lecture:\n",
    "\n",
    "- `coef` : $\\beta_1$ estimate explaining the effect size\n",
    "- `std err` : standard error \n",
    "- `P>|t|` : the p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f512b-d735-4f2d-97f0-8867c6851131",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c4896-926d-482d-900a-7b00d72796b4",
   "metadata": {},
   "source": [
    "$$outcome = \\beta_0 + \\beta_1*predictor$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2584812a-968f-4380-9f3d-8e4860caff6f",
   "metadata": {},
   "source": [
    "$$ Teen Birth = 15.67 + 2.03  * Poverty Percentage $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9630dc-01f7-4072-bb55-ddc115138f00",
   "metadata": {},
   "source": [
    "If the Poverty Percentage were 0, the Teen Birth Rate would be **15.67** (The Intercept, $\\beta_0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8610c49d-3be3-4e2c-b35f-0f44b7888b54",
   "metadata": {},
   "source": [
    "For every 1 unit increase in Poverty Percentage, you expect to see a **2.03** unit increase in Teen Birth Rate (The effect size, $\\beta_1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bcac76-205b-44ce-94c3-3dec48b684bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7989eaa-242e-4b06-afac-6f159a78dc56",
   "metadata": {},
   "source": [
    "### Question #2\n",
    "\n",
    "Which value represents the expected Teen Birth Rate if the Poverty Percentage were 0?\n",
    "\n",
    "- A) 15.67\n",
    "- B) 2.03\n",
    "- C) 4.032\n",
    "- D) 0.495"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf51d9-f9fc-4fa8-8bbb-5b169ed0453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the orginal data (as before)\n",
    "sns.scatterplot(x='PovPct', y='TeenBrth', data=df, s=100)\n",
    "\n",
    "# Generate and plot the model fit line\n",
    "xs = np.arange(df['PovPct'].min(), df['PovPct'].max())\n",
    "ys = results.params.iloc[1] * xs + results.params.iloc[0]\n",
    "plt.plot(xs, ys, '--k', linewidth=4, label='Model')\n",
    "\n",
    "# make the title tell us model parameters\n",
    "vals='$y={:.3f}x + {:.3f}, p={:.3f}, r={:.3f}, r^2={:.3f}$'.format(results.params.iloc[0], \n",
    "                                                                   results.params.iloc[1], \n",
    "                                                                   results.pvalues.iloc[1], \n",
    "                                                                   df[['PovPct','TeenBrth']].corr().iloc[0,1], \n",
    "                                                                   results.rsquared)\n",
    "plt.title(vals);\n",
    "\n",
    "plt.xlabel('Poverty Percentage')\n",
    "plt.ylabel('Teen Birth Rate')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de6702c-ea80-4f1c-b00b-f8fc748fdfac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "090f7b3a-7253-4276-a49a-9a2b105feb86",
   "metadata": {},
   "source": [
    "The model (the line) mathematically describes the relationship between the data points, but it doesn't explain the relationship *perfectly*. (All models are wrong!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f2a127-63db-4901-beef-22c7b8e2db2f",
   "metadata": {},
   "source": [
    "Note that this line of regression comes from minimizing the Residual Sum of Squares (RSS):\n",
    "- if you were to draw a perpendicular line from each point to the line and calculate that distance, that is the residual\n",
    "- The RSS is if you square that residual and then sum across all points\n",
    "- this line of regression is the slope/intercept that minimizes RSS.\n",
    "- the method of minimizing RSS is called Ordinary Lease Squares or OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d3004-f304-435b-873c-be98bbd5a9e3",
   "metadata": {},
   "source": [
    "## Estimates\n",
    "\n",
    "If I told you a locations' Poverty Percentage, what would you guess its Teen Birth Rate would be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96606a5b-d1d0-483b-bfca-19ef62cee93c",
   "metadata": {},
   "source": [
    "### Clicker Question #3\n",
    "\n",
    "If I told you the Poverty Percentage of a state was 15, what would you estimate would be its Teen Birth Rate? \n",
    "- A) ~ 0\n",
    "- B) ~ 30\n",
    "- C) ~ 40\n",
    "- D) ~ 50\n",
    "- E) ~ 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbeff5-4224-4098-a7a8-0606700fb6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abc53849-484e-4168-8d73-4bf51c9a9aec",
   "metadata": {},
   "source": [
    "### Clicker Question #4\n",
    "\n",
    "Which of the following is a reasonable estimate (guess) for a state with a Poverty Percentage of 20?\n",
    "\n",
    "- A) 55.99\n",
    "- B) 56.27\n",
    "- C) 56.5\n",
    "- D) A - C\n",
    "- E) None of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df75990-b595-4a85-a996-7d481e72481e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8d7d1c8-d564-4659-9c8d-a2b0c4d53b75",
   "metadata": {},
   "source": [
    "### Clicker Question #5\n",
    "\n",
    "What is our conclusion from this analysis? (Question: Does Poverty Percentage affect Teen Birth Rate?)\n",
    "\n",
    "- A) Reject the null; There is no relationship between Poverty Percentage and Teen Birth Rate\n",
    "- B) Reject the null; There is a relationship between Poverty Percentage and Teen Birth Rate\n",
    "- C) Fail to reject the null; There is no relationship between Poverty Percentage and Teen Birth Rate\n",
    "- D) Fail to reject the null; There is a relationship between Poverty Percentage and Teen Birth Rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a76a30-55e7-440d-ba07-2a9e4626417e",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "So far, we've only been looking at the relationship of one measure (Poverty Percentage) on Teen Birth. \n",
    "\n",
    "*But*, Poverty Percentage is likely *not* the only thing that affects Teen Birth Rate.\n",
    "\n",
    "We could imagine that Violent Crime rates in a location may affect both Poverty Percentage and could possibly affect Teen Birth Rate. (A confounder!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e8bd5-5a4f-4865-9131-8413b91e64b0",
   "metadata": {},
   "source": [
    "This is where **multiple linear regression** is incredibly helpful. Multiple linear regression allows you to measure the effect of multiple predictors on an outcome.\n",
    "\n",
    "Let's do some EDA first to see if our hypothesis (Violent Crime is related to Teen Birth) holds up in our data at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9e738-6ec3-42db-9aaf-8948c19ce211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship between predictors\n",
    "sns.scatterplot(x='PovPct', y='ViolCrime', data=df, s=100)\n",
    "plt.xlabel('Poverty Percentage')\n",
    "plt.ylabel('Violent Crime');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bba027-a8a4-41f4-9a32-4a22402b8b55",
   "metadata": {},
   "source": [
    "### Outlier handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d1617f-6da4-47a3-988e-18af9b3a788e",
   "metadata": {},
   "source": [
    "Ugh! there's some nasty outlier going on there.... if we remove it maybe we will be able to see some relationship but we can't right now because that one huge violent crime location is messing up the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3545744-d4e9-46bd-a3e7-b93b5dce42b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['ViolCrime'] > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e2fa6-68af-4db7-8e6b-53e98d2ad330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing DC\n",
    "df = df[df['ViolCrime'] < 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa84bb-c7f7-40da-a84e-019325d1f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship between predictors\n",
    "# after removing outlier\n",
    "sns.scatterplot(x='PovPct', y='ViolCrime', data=df, s=100)\n",
    "plt.xlabel('Poverty Percentage')\n",
    "plt.ylabel('Violent Crime');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252cc798-f1ed-4be9-be17-2c550872ff47",
   "metadata": {},
   "source": [
    "Yay, now we can see what looks like a potentially linear relationship between poverty and violent crime.\n",
    "\n",
    "But BEWARE!  Removing an outlier WILL change the coefficients of the regression!\n",
    "\n",
    "We did it here because the outlier was so big it might destroy the linear relationship.  But that's a judgement call, there is no \"right way\" when it comes to outlier removal.\n",
    "\n",
    "Maybe if you've got time, you should rerun the analysis below WITHOUT removing DC... see what's different :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea02797-5512-499c-83dd-98bf68cc9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mod = smf.ols(formula='TeenBrth ~ ViolCrime', data=df)\n",
    "resvc = mod.fit()\n",
    "print(resvc.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb65a6e2-7dae-4c70-b6b2-9e5ece880788",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mod = smf.ols(formula='TeenBrth ~ PovPct + ViolCrime', data=df)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d6464a-5e99-4036-9692-d1c160efa898",
   "metadata": {},
   "source": [
    "### Clicker Question #6\n",
    "\n",
    "Which has a larger effect on Teen Birth Rate?\n",
    "\n",
    "- A) Poverty Percentage\n",
    "- B) Violent Crime\n",
    "- C) Effect is equal across all predictors\n",
    "- D) No predictors have an effect on outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485eeb0b-d24c-483b-b3c1-e89cf622d4a6",
   "metadata": {},
   "source": [
    "$$ Teen Birth = 15.3 + (1.19  * Poverty Percentage) + (1.63 * Violent Crime) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db9458-89fa-4af4-bb71-82deaaf3138a",
   "metadata": {},
   "source": [
    "If the Poverty Percntage *and* Violent Crime were both 0, the Teen Birth Rate would be **15.3** (The Intercept, aka $\\beta_0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3dfe87-7030-43d4-bffb-a50d1253c7c0",
   "metadata": {},
   "source": [
    "Holding Violent crime constant, for every 1 unit increase in Poverty Percentage, you expect to see a **1.9** unit increase in Teen Birth Rate (The effect size, $\\beta_1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2898a3-a9ae-46b9-bcb6-6e8b91f9a5c7",
   "metadata": {},
   "source": [
    "Holding Poverty Percentage constant, for every 1 unit increase in Violent crime, you'd expect to see a **1.63** unit increase in Teen Birth Rate (The effect size, $\\beta_2$)\n",
    "\n",
    "\n",
    "## Model selection\n",
    "\n",
    "We've now seen 3 different models\n",
    "1. TB ~ PP\n",
    "2. TB ~ VC\n",
    "3. TB ~ PP + VC\n",
    "\n",
    "Let's review the different model fits... hopefully they contain a way to select which model best fits the data! (spoiler, they do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc32621e-2ca8-4d18-8d2a-60c62182b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summary())\n",
    "print(resvc.summary())\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8de2a5-7483-41ed-864c-b658c2de21ba",
   "metadata": {},
   "source": [
    "Much of the right hand column (R-squared through BIC) of the summary table are methods of *in-sample* model selection.\n",
    "\n",
    "In-sample means that these are measures meant to be used when you are evaluating how good a model is using the same data you are using to fit the model.  In-sample stands in contrast to out-of-sample methods you might have heard of like cross-validation (which uses different subsets of the data to fit a model than it does when selecting which model to use)\n",
    "\n",
    "Lets start with R-squared.  R-squared is a **goodness-of-fit** measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.  We already know that Pearson's r having a value close to +1/-1 means the datapoints lie basically on a straight line.  When $r=1$the dots fall perfectly on the line, and knowing X uniquely determines Y... there is no noise.  When r is closer to zero noise is most of what we are seeing and knowing X doesn't tell us much about the corresponding Y value.  R-squared is literally Pearson's r, squared.  So it has the same properties plus it can be literally interpreted as \"percent of the variance of Y which is explained by knowing X\", AKA \"percent variance explained\". Models with high variance explained are models that fit their data well.\n",
    "\n",
    "But selecting a model is MORE than just \"did it fit the data well\". \n",
    "\n",
    "To do in-sample model selection you want ignore R-squared and instead look at ANY of the following metrics\n",
    "- Adj. R-squared\n",
    "- AIC\n",
    "- BIC\n",
    "\n",
    "Going into the math details of these different measures here is beyond the scope of this notebook.  For a full treatment I suggest Chapter 6 of the free textbook https://www.statlearning.com.  For a quick intro I found these slides useful https://pages.stat.wisc.edu/~ane/st572/notes/lec05.pdf\n",
    "\n",
    "For an ever quicker introduction you need to understand an important principle.... the parsimony principle.  Parsimony is the idea (AKA, Occam's razor) that if two models fit the data equally well (e.g., have the roughly the same R-squared) then we should prefer the simpler model.  How is one model simpler than the others?  A simpler model has fewer X variables than a complex one.  If we try to fit a line through 10 datapoints in 2 dimensions then it has 2 parameters (slope/intercept) and that's pretty simple.  But if we try to fit a 9th order polynomial (it has terms with $x^9, x^8, \\ldots, x$ plus an intercept) then we will *perfectly* hit all 10 datapoints with zero RSS.  But this wiggly curve will NOT generalize well to new data... we refer to this an overfit model because it is overly complex and fitting noise in the data as if it was signal.\n",
    "\n",
    "Adjusted R-squared, AIC, and BIC are all metrics that are related to each other (again math details elsewhere).  These metrics reflect how good a model is at fitting data, but they also have complexity penalties built into them.  If two models fit the data the same, but one is more complex these metrics will prefer the simpler model.\n",
    "\n",
    "Adjusted R-squared goes UP for better fit/simpler models and DOWN for worse fit/more complex models.\n",
    "\n",
    "AIC and BIC do the opposite.  They go DOWN for better fit/simpler models and UP for worse fit/more complex models.\n",
    "\n",
    "Any of these metrics can be used to select among different potential models (e.g., our 3 models above).  Generally BIC hates complex models more than the other two if you're looking to penalize model complexity.\n",
    "\n",
    "So let's compare our 3 models using BIC from the results tables above.... which model do you prefer?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644363be-a64e-4ada-9293-c6434a9a137b",
   "metadata": {},
   "source": [
    "## Regressing categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c7aee-3d29-4263-bd38-2fb3b78e1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['ViolCrime'], bins=10)\n",
    "plt.xlabel('Violent Crime');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb5742-18b6-4841-8e5a-0e59b1f842c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ViolCrime'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6a7f8-f942-427b-a737-ce658f122a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new categorical column for a 'Violent' state, arbitrarily pick a threshold from histogram\n",
    "df['Violent'] = df['ViolCrime'].apply(lambda x: 0 if x < 11 else 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6dbdc4-b3af-453a-ac9b-122f1722dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Violent', y='TeenBrth', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d514d9be-586c-4bea-bf36-2f68d542e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula='TeenBrth ~ PovPct + C(Violent)', data=df)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb07c7-6f12-4d86-9dfc-fe67b3f34582",
   "metadata": {},
   "source": [
    "# Difference in means: t-test\n",
    "\n",
    "The t-test tests for a difference in means between two groups.  Now that we have a categorical variable we can use a t-test to ask if there is a difference between violent states and non-violent ones in terms of teen birth rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e988f-137f-4986-9946-28a7efdb3db3",
   "metadata": {},
   "source": [
    "Student's t-test assumptions:\n",
    "\n",
    "- Data are continuous\n",
    "- Normally distributed\n",
    "- Large enough sample size\n",
    "- Equal variance b/w groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec39e7c-ba9a-4fad-82a9-b33827748601",
   "metadata": {},
   "source": [
    "$H_0: \\bar x = \\bar y$\n",
    "\n",
    "$H_a: \\bar x \\ne \\bar y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8241bb11-9bb0-41ec-944f-a2c24b58ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Violent'])['TeenBrth'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bf223a-f9da-471f-b9dc-7e59b15de6d8",
   "metadata": {},
   "source": [
    "$H_0: \\bar x_{ViolentTeenBirthRate} = \\bar y_{NotViolentTeenBirthRate}$\n",
    "\n",
    "$H_a: \\bar x_{ViolentTeenBirthRate} \\ne \\bar y_{NotViolentTeenBirthRate}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98654da9-3b47-4a30-9423-499d79ddfd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = df.where(df.Violent==0).dropna()['TeenBrth']\n",
    "group2 = df.where(df.Violent==1).dropna()['TeenBrth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba2c63-bc99-4715-bf02-edcf96653ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(group1, group2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67176bec-3093-4629-90c6-250c55a784f5",
   "metadata": {},
   "source": [
    "The p-value here is $0.02$ ...which means that observing this difference in means due to random chance alone is unlikely. \n",
    "\n",
    "By standard inferential threshold of $p=0.05$ we could reject the null in favor of the alternative hypothesis, concluding that the means between the groups are not the same and that violence is associated with an increase in Teen Birth Rate.\n",
    "\n",
    "I would personally tend to just report the p value, and say this difference is likely not due to chance rather than using language like \"reject the null\".\n",
    "\n",
    "NOW bad news... if we pick a different threshold for violence in a state, or if we kept DC in the data these values (and conclusion) would probably change!!! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50926d-5383-421d-b863-1eef447c47ba",
   "metadata": {},
   "source": [
    "# Multicolinearity \n",
    "Lets force a colinearity problem by making a synthetic variable to show you what to look out for...\n",
    "\n",
    "specifically note that \n",
    "1. condition number is large... that's diagnosing the issue\n",
    "2. coefficient on violent crime changes dramatically from before\n",
    "3. coefficient on psuedoVC is similar in magnitude to violent crime, robbing it of the explanatory power it deserves\n",
    "4. the sum of coefficients doesn't arrive at the total it was at in previous model... again robbing it of explanatroy power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8735d1-4130-4cd8-8ea3-042c848cbb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['psuedoVC'] = df['ViolCrime'] * 3.6 + 112 + 5*np.random.rand(len(df))\n",
    "mod = smf.ols(formula='TeenBrth ~ PovPct + ViolCrime + psuedoVC', data=df)  \n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb3be02-bb5f-411a-9168-02bc5d509f40",
   "metadata": {},
   "source": [
    "# Regression with transformed data\n",
    "\n",
    "So far, we've been working with data that were approximately Normal and didn't require transformation. But that won't always be the case..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec09a7-0e7b-4a7e-a330-cc4d109c8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep = pd.read_csv('https://raw.githubusercontent.com/shanellis/datasets/master/msleep.csv')\n",
    "sleep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca93309-6a36-4d14-8c2e-3c05a923de82",
   "metadata": {},
   "source": [
    "What if we were interested in quantifying the **effect that REM sleep has on total sleep**?\n",
    "\n",
    "We may hypothesize that the more REM sleep an animal gets the less total sleep it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ff69b-298b-41fe-952c-639ba6b0ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sleep['sleep_rem'].dropna())\n",
    "plt.xlabel('REM Sleep (hours/night)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436c5dec-7e83-4349-a8dc-95d865dd5ca2",
   "metadata": {},
   "source": [
    "These data are skewed right, and transforming these data could help us use them in analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1f425-716a-47cb-87d1-8a0718c1a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='sleep_rem', y='sleep_total', data=sleep, s=100)\n",
    "plt.xlabel('REM Sleep')\n",
    "plt.ylabel('Total Sleep');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efcfafb-216c-4f1d-9f32-11fbbff4abaa",
   "metadata": {},
   "source": [
    "This relationship is not linear...so linear regression would not be appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b5c040-b9ea-4895-94fa-2cba4e747780",
   "metadata": {},
   "source": [
    "### Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03249dc2-225b-435b-ad50-fd6e4b1a4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add log-transofmed column to sleep_rem & brainwt\n",
    "sleep['sleep_rem10'] = np.log10(sleep['sleep_rem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1416601-93e4-4adc-bde6-a8f82ad9cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this now looks less skewed, more bell shaped\n",
    "sns.histplot(sleep['sleep_rem10'].dropna())\n",
    "plt.xlabel('log10(REM Sleep)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727e203-22bd-4f4b-a21d-036a11005850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and therefore, this now looks closer to a linear relationship\n",
    "sns.scatterplot(x='sleep_rem10', y='sleep_total', data=sleep, s=100)\n",
    "plt.xlabel('log10(REM Sleep)')\n",
    "plt.ylabel('Total Sleep');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b9210-df31-4c7f-918f-9a5b45f6ead6",
   "metadata": {},
   "source": [
    "### Linear regression with transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6141118-3317-4395-9dd3-6648d4b1aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carry out regression using log-transformed predictor\n",
    "\n",
    "mod = smf.ols(formula='sleep_total ~ sleep_rem10', data=sleep)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495148d-3a88-4223-8471-54e3c154b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view our log transformed lmplot\n",
    "sns.lmplot(x='sleep_rem10',y='sleep_total',data=sleep);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c07ec31-531e-4975-a60d-baacfe6df135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare above to the lmplot without log transformation\n",
    "sns.lmplot(x='sleep_rem',y='sleep_total',data=sleep);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ec4495-7423-455a-9332-edf41b9a3ab1",
   "metadata": {},
   "source": [
    "Here, we see that the $\\beta_1$ estimate for `sleep_rem10` is 10.89....but remember that this value is on the log scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3280e6-0fc9-4171-a1fe-8c546b88ca62",
   "metadata": {},
   "source": [
    "$$ Total Sleep = 8.57 + 10.89  * log_{10}(REM Sleep) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea2a970-3ab3-41f0-a6a2-69679d7202a8",
   "metadata": {},
   "source": [
    "To interpret this coefficient, we would say that\n",
    "\n",
    "a 1 unit increase in REM sleep, Total Sleep increases by $\\approx$ $10.89/100$ units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3b0a44-304f-45f6-aee3-01c04ff02c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpreting a log transformed coefficient\n",
    "10.89 / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401fbe80-bddc-47d8-b7e8-b479e3329ea7",
   "metadata": {},
   "source": [
    "So, for each 1 hour increase in REM Sleep, Total Sleep increases by 0.11 hours. \n",
    "\n",
    "That's not a large effect, but it is different than what we hypothesized at the beginning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2253c922-b800-477f-8e82-308c6b71f32e",
   "metadata": {},
   "source": [
    "More on interpretation of log transformed variables can be read [here](https://www.cscu.cornell.edu/news/statnews/stnews83.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad93979d-707b-4d30-b3e1-4f6b9d9d5782",
   "metadata": {},
   "source": [
    "### Why is log-transformation not math trickery?\n",
    "\n",
    "First, some reading on data transformation can be found [here](http://fmwww.bc.edu/repec/bocode/t/transint.html). It's not beautifully formatted, but I think it is written in a more-straightforward manner than some [other](https://stats.stackexchange.com/questions/4831/regression-transforming-variables/4833#4833) [places](https://stats.stackexchange.com/questions/298/in-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-va) [online](https://www.researchgate.net/post/Why_do_we_do_transformation_before_data_analysis).\n",
    "\n",
    "But a basic understanding can be had in just a few bullet points\n",
    "- measurement scales may be arbitrary. there's nothing to say log transforms aren't a better measurement scale than the original\n",
    "- you are already familiar with many logrithmic measurement scales: decibels and Richter scale for measuring earthqueakes are two examples\n",
    "- lots of things are power-law distributed or close to power-law distributed... the frequency of words in a language, human vision and auditory abilities in perception, the number of social media followers people have. \n",
    "- log transforms make a process that had been multiplicative or power law like ($a*b^n$) into an additive process ($a+n*b$) which really helps when you're trying to apply linear regression for obvious reasons\n",
    "- log transforms can reduce heteroscadacity and skewness, which may make the data amenable to linear regression when it hadn't been before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a830da94-d56e-4679-bd0f-eda3115c4ac6",
   "metadata": {},
   "source": [
    "The first thing to remember is there is (in most cases) nothing special about how the data are originally expressed.\n",
    "\n",
    "In our example above, there is nothing about \"hours\" as the unit that was chosen that makes these data \"correct\".\n",
    "\n",
    "So, while it _feels_ like data transformation is trickery, our initial unit of hours is...in some ways arbitrary and something that we chose.\n",
    "\n",
    "This is where we'll start with our argument that it's ok to transform (or think of it as *re-express*) our data\n",
    "so that it can be (still-accurately) used with well-studied models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab32b28-a2ed-4e79-b3c0-5b1425a35275",
   "metadata": {},
   "source": [
    "To make the point a little more concretely, all the following things are measured on a log scale: pH (measurement of acidity), dB (measure of sound loudness), and Richter scale (earthquake intensity). All of them _could_ be measured  on a linear scale. Those measurements would still explain those things...it would just be on a different scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4943cddb-11c4-46b6-891f-01ce1ba22e46",
   "metadata": {},
   "source": [
    "In other words:\n",
    "    \n",
    "> \"Transformations are needed because there is no guarantee that the world works on the scales it happens to be measured on.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b48c8-d966-4b79-9796-4052da9bdbbd",
   "metadata": {},
   "source": [
    "What *does* differ however, is the interprtation. Linear scales tell us absolute change, while logarithmic scales tell us relative change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec10aac-4d58-484a-aa5e-1ac910ee3d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0df0731b-8af9-4c25-835e-85031c8ab0e8",
   "metadata": {},
   "source": [
    "# Differences in means by ANOVA\n",
    "\n",
    "What if we have more than two possible levels in our categorical variable?  T-tests are for only two things, e.g. male/female, violent/non-violent.  What if we have regions of the US, like midwest, west, northeast... there's more than two of those!\n",
    "\n",
    "Thats where a group-wise test like ANOVA (Analysis of Variance) is perfect\n",
    "\n",
    "Parametric ANOVA test assumptions\n",
    "\n",
    "- Population distributions are normal\n",
    "- Samples have equal variances\n",
    "- Independence\n",
    "\n",
    "Hypothesis\n",
    "\n",
    "$H_0: \\bar x_1 = \\bar x_2 = \\bar x_3 \\ldots $\n",
    "\n",
    "$H_a: $ at least one group mean is different \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad257d0-7157-4dc7-9666-d73018a4f73b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regions = pd.read_csv('https://raw.githubusercontent.com/cphalpert/census-regions/master/us%20census%20bureau%20regions%20and%20divisions.csv')\n",
    "regions = regions.rename({'State':'Location'},axis=1).set_index('Location')\n",
    "regions.index = regions.index.str.replace(' ','_') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6585d-454b-42b9-84ee-048ede87158f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fda2f-5787-42b0-87a3-ef87eb40e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(regions['Region'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f2c430-c272-4f93-ade1-c07d25454d14",
   "metadata": {},
   "source": [
    "So how would you run an ANOVA checking for a difference in TeenBrth across regions?\n",
    "\n",
    "this is left as an exercise to the reader. But there are two major options for you.  If you want to manually group the categorical variable ($\\bar x_1 = \\bar x_2 = \\bar x_3$) you can use scipy:\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html\n",
    "\n",
    "If you want to build a model that is more precise or custom, with automatic grouping by a variable inside the dataframe then you can use statsmodels:\n",
    "https://www.statsmodels.org/dev/anova.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab5e74e-f0c8-4eda-a173-54aa4af9e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions.index = regions.index.str.replace(' ','_') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58737798-2133-46aa-994f-8e4acfb100ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
